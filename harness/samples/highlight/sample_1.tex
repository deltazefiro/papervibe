\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}

\section{MovieChat}

\subsection{Visual Feature Extraction}
For visual feature extraction, instead of utilizing video-based foundational models such as ViViT or Video-Swin, we simply use an image-based model to get frame-wise feature in the form of tokens. To be specific, we utilize pre-trained models as our visual feature extractor, including the ViT-G/14 from EVA-CLIP and the Q-former from BLIP-2. This is mainly because 1) there is few video foundation model that makes good alignment with text, and 2) our proposed memory mechanism can effectively capture temporal features. Given a raw video, the visual input $\mathbf{v} \in \mathbb{Z}^{T \times 3 \times H \times W}$ is a sequence of $T$ RGB frames of size $H \times W$ sampled from the video. The visual features are extracted in a sliding window manner, which could be formulated as
\begin{equation}
    B_{n} = \{ \mathbf{x}_{i}= \mathcal{V}(\mathbf{v}_{i}) \mid \forall i = 1,...,C\}, n = 1,...,\lceil \frac{T}{C} \rceil,
\end{equation}
where $B_{n}$ is the $n$-th video clip feature within the sliding window spanning $C$ frames. $\mathcal{V}(\cdot)$ is the visual feature extractor, taking as input a single frame {$\mathbf{v}_{i}\in \mathbb{Z}^{3 \times H \times W}$}. {$\mathbf{x}_{i}$} $\in \mathbb{R}^{N \times D}$ denotes $N$ extracted visual tokens with respect to each frame, and $D$ is the feature dimension of each token.

\vspace{2.5pt}

\subsection{Short-term Memory}
Short-term memory stores the frame tokens in a temporary fixed-length buffer. The previously extracted visual features by sliding window $G$ times without further processing are used to construct short-term memory, which can be formulated by:
\begin{equation}
    \mathcal{S} = \bigcup_{n}{B}_{n} = \{ \mathbf{{x}}_{i} \mid \forall i = 1, ..., K\}, n=1,..,G,
\end{equation}
where $\mathcal{S}$ is short-term memory, and $K$ is equal to $ C \times G$. Note that we set short-term memory to contain a fixed length of $K$ frames since the role of short-term memory is to assist in video understanding based on previous short-term contextual information. 

The update strategy for short-term memory is based on the First-in-First-out~(FIFO) queue. As a new batch of visual tokens enters, when the short-term memory reaches its capacity, we pop the currently stored frames to the memory consolidation module and clear the short-term memory. The output video feature obtained from the consolidation module augments the long-term memory; on the other hand, it reinitializes the short-term memory with this feature. The initialization aims at communicating the information between different sliding windows, thereby achieving more efficient compression.

\vspace{2.5pt}

\end{document}
